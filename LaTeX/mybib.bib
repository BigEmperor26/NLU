%% Created for Prasanta Ghosh on -01-20

@misc{
	lin2022a,
	title={A Closer Look at Loss Weighting in Multi-Task Learning},
	author={Baijiong Lin and Feiyang YE and Yu Zhang},
	year={2022},
	url={https://openreview.net/forum?id=OdnNBNIdFul},
}
@inproceedings{wang-etal-2018-bi,
	title = "A Bi-Model Based {RNN} Semantic Frame Parsing Model for Intent Detection and Slot Filling",
	author = "Wang, Yu  and
	Shen, Yilin  and
	Jin, Hongxia",
	booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
	month = jun,
	year = "2018",
	address = "New Orleans, Louisiana",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N18-2050",
	doi = "10.18653/v1/N18-2050",
	pages = "309--314",
	abstract = "Intent detection and slot filling are two main tasks for building a spoken language understanding(SLU) system. Multiple deep learning based models have demonstrated good results on these tasks . The most effective algorithms are based on the structures of sequence to sequence models (or {``}encoder-decoder{''} models), and generate the intents and semantic tags either using separate models. Most of the previous studies, however, either treat the intent detection and slot filling as two separate parallel tasks, or use a sequence to sequence model to generate both semantic tags and intent. None of the approaches consider the cross-impact between the intent detection task and the slot filling task. In this paper, new Bi-model based RNN semantic frame parsing network structures are designed to perform the intent detection and slot filling tasks jointly, by considering their cross-impact to each other using two correlated bidirectional LSTMs (BLSTM). Our Bi-model structure with a decoder achieves state-of-art result on the benchmark ATIS data, with about 0.5{\%} intent accuracy improvement and 0.9 {\%} slot filling improvement.",
}
@misc{https://doi.org/10.48550/arxiv.1902.10909,
	doi = {10.48550/ARXIV.1902.10909},
	
	url = {https://arxiv.org/abs/1902.10909},
	
	author = {Chen, Qian and Zhuo, Zhu and Wang, Wen},
	
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {BERT for Joint Intent Classification and Slot Filling},
	
	publisher = {arXiv},
	
	year = {2019},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{qin-etal-2019-stack,
	title = "A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding",
	author = "Qin, Libo  and
	Che, Wanxiang  and
	Li, Yangming  and
	Wen, Haoyang  and
	Liu, Ting",
	booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D19-1214",
	doi = "10.18653/v1/D19-1214",
	pages = "2078--2087",
	abstract = "Intent detection and slot filling are two main tasks for building a spoken language understanding (SLU) system. The two tasks are closely tied and the slots often highly depend on the intent. In this paper, we propose a novel framework for SLU to better incorporate the intent information, which further guiding the slot filling. In our framework, we adopt a joint model with Stack-Propagation which can directly use the intent information as input for slot filling, thus to capture the intent semantic knowledge. In addition, to further alleviate the error propagation, we perform the token-level intent detection for the Stack-Propagation framework. Experiments on two publicly datasets show that our model achieves the state-of-the-art performance and outperforms other previous methods by a large margin. Finally, we use the Bidirectional Encoder Representation from Transformer (BERT) model in our framework, which further boost our performance in SLU task.",
}
@article{Davis80-COP,
	Author = {Steven B. Davis and Paul Mermelstein},
	Journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
	Number = {4},
	Pages = {357--366},
	Title = {Comparison of Parametric Representation for Monosyllabic Word Recognition in Continuously Spoken Sentences},
	Volume = {28},
  Month = aug,
	Year = {1980}}

@article{Rabiner89-ATO,
	Author = {Lawrence R. Rabiner},
	Journal = {Proceedings of the IEEE},
	Number = {2},
	Pages = {257--286},
	Title = {A Tutorial on Hidden {Markov} Models and Selected Applications in Speech Recognition},
	Volume = {77},
  Month = feb,
	Year = {1989}}

@book{Hastie09-TEO,
	Address = {New York},
	Author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
	Publisher = {Springer},
	Title = {The Elements of Statistical Learning -- Data Mining, Inference, and Prediction},
	Year = {2009}}

@inproceedings{YourName21-XXX,
	Author = {Firstname1 Lastname1 and Firstname2 Lastname2 and Firstname3 Lastname3},
	Crossref = {INTERSPEECH21},
	Pages = {100--104},
	Title = {Title of your {INTERSPEECH} 2021 publication}}

@proceedings{INTERSPEECH21,
	Booktitle = {Proceedings {INTERSPEECH} 2021 -- 22\textsuperscript{th} Annual Conference of the International Speech Communication Association},
	Date-Modified = {2021-01-22},
	Key = {INTERSPEECH},
	Title = {Proceedings {INTERSPEECH} 2021 -- 22\textsuperscript{th} Annual Conference of the International Speech Communication Association},
  Address = {{Brno, Czech Republic}},
  Month = {{Aug./Sep.}},
	Year = {2021}}
